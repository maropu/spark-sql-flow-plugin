#!/usr/bin/env bash

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

set -e -o pipefail

# Sets the root directory
ROOT_DIR="$(cd "`dirname $0`"/..; pwd)"

# Loads some variables from `pom.xml`
. ${ROOT_DIR}/bin/package.sh && get_package_variables_from_pom "${ROOT_DIR}"

# Splits input arguments into two parts: pyspark confs and args
parse_args_for_pyspark() {
  SPARK_CONF=()
  ARGS=()
  while [ ! -z "$1" ]; do
    if [[ "$1" =~ ^--master= ]]; then
      SPARK_CONF+=($1)
    elif [ "$1" == "--conf" ]; then
      shift
      SPARK_CONF+=("--conf $1")
    else
      ARGS+=($1)
    fi
    shift
  done
}

find_package() {
  local _BUILT_PACKAGE="${ROOT_DIR}/target/${PACKAGE_JAR_NAME}"
  if [ -e "$_BUILT_PACKAGE" ]; then
    PACKAGE=$_BUILT_PACKAGE
  else
    PACKAGE="${ROOT_DIR}/assembly/${PACKAGE_JAR_NAME}"
    echo "${_BUILT_PACKAGE} not found, so use pre-compiled ${PACKAGE}" 1>&2
  fi
}

# Joins an input array by a given separator
join_by() {
  local IFS="$1"
  shift
  echo "$*"
}

# Do some preparations before launching pyspark
parse_args_for_pyspark "$@"
find_package

# Activate a conda virtual env
. ${ROOT_DIR}/bin/conda.sh && activate_conda_virtual_env "${ROOT_DIR}"

# Then, launches a pyspark with given arguments
PYTHONPATH="${ROOT_DIR}/python" \
PYTHONSTARTUP="${ROOT_DIR}/bin/.startup.py" \
  exec pyspark --jars=${PACKAGE} $(join_by " " ${SPARK_CONF[@]}) $(join_by " " ${ARGS[@]})

